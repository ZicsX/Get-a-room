{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that are not needed\n",
    "train.drop(['Property_ID'], axis=1, inplace=True)\n",
    "testPrpId = y = test['Property_ID']\n",
    "test.drop(['Property_ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X and y\n",
    "y = train['Habitability_score']\n",
    "train.drop(['Habitability_score'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting string columns to numeric\n",
    "categorical_features = ['Property_Type', 'Furnishing', 'Frequency_of_Powercuts', 'Power_Backup', 'Water_Supply', 'Crime_Rate', 'Dust_and_Noise']\n",
    "labelencoder = LabelEncoder()\n",
    "for feature in categorical_features:\n",
    "    train[feature] = train[feature].astype(str)\n",
    "    test[feature] = test[feature].astype(str)\n",
    "\n",
    "    labelencoder.fit(train[feature])\n",
    "\n",
    "    train[feature] = labelencoder.transform(train[feature])\n",
    "    test[feature] = labelencoder.transform(test[feature])\n",
    "\n",
    "    train[feature].fillna(train[feature].mean(), inplace = True)\n",
    "    test[feature].fillna(train[feature].mean(), inplace = True)\n",
    "\n",
    "# impute missing values\n",
    "numerical_features = ['Property_Area', 'Number_of_Windows', 'Number_of_Doors','Traffic_Density_Score','Air_Quality_Index','Neighborhood_Review']\n",
    "for feature in numerical_features:\n",
    "    train[feature].fillna(train[feature].mean(), inplace = True)\n",
    "    test[feature].fillna(test[feature].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Property_Type</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Number_of_Windows</th>\n",
       "      <th>Number_of_Doors</th>\n",
       "      <th>Furnishing</th>\n",
       "      <th>Frequency_of_Powercuts</th>\n",
       "      <th>Power_Backup</th>\n",
       "      <th>Water_Supply</th>\n",
       "      <th>Traffic_Density_Score</th>\n",
       "      <th>Crime_Rate</th>\n",
       "      <th>Dust_and_Noise</th>\n",
       "      <th>Air_Quality_Index</th>\n",
       "      <th>Neighborhood_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>3.923768</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5.89</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>733</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.37</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>96.0</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>737</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7.45</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>121.0</td>\n",
       "      <td>3.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>900</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6.16</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2238</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.46</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>116.0</td>\n",
       "      <td>4.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Property_Type  Property_Area  Number_of_Windows  Number_of_Doors  \\\n",
       "0              1            106           3.923768                1   \n",
       "1              1            733           2.000000                2   \n",
       "2              1            737           4.000000                2   \n",
       "3              1            900           3.000000                2   \n",
       "4              2           2238          14.000000                6   \n",
       "\n",
       "   Furnishing  Frequency_of_Powercuts  Power_Backup  Water_Supply  \\\n",
       "0           1                       0             1             3   \n",
       "1           2                       1             1             2   \n",
       "2           0                       0             1             3   \n",
       "3           2                       2             2             3   \n",
       "4           0                       0             1             0   \n",
       "\n",
       "   Traffic_Density_Score  Crime_Rate  Dust_and_Noise  Air_Quality_Index  \\\n",
       "0                   5.89           1               2               90.0   \n",
       "1                   4.37           3               2               96.0   \n",
       "2                   7.45           1               2              121.0   \n",
       "3                   6.16           2               2              100.0   \n",
       "4                   5.46           3               2              116.0   \n",
       "\n",
       "   Neighborhood_Review  \n",
       "0                 3.86  \n",
       "1                 3.55  \n",
       "2                 3.81  \n",
       "3                 1.34  \n",
       "4                 4.77  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Property_Type                6\n",
       "Property_Area             4435\n",
       "Number_of_Windows           17\n",
       "Number_of_Doors              6\n",
       "Furnishing                   4\n",
       "Frequency_of_Powercuts       5\n",
       "Power_Backup                 3\n",
       "Water_Supply                 5\n",
       "Traffic_Density_Score      772\n",
       "Crime_Rate                   5\n",
       "Dust_and_Noise               4\n",
       "Air_Quality_Index          489\n",
       "Neighborhood_Review        417\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique values in each column\n",
    "train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for categorical variables\n",
    "categorical_features = ['Property_Type', 'Furnishing', 'Frequency_of_Powercuts', 'Power_Backup', 'Water_Supply', 'Crime_Rate', 'Dust_and_Noise']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# pipeline for numerical variables\n",
    "numerical_features = ['Property_Area', 'Number_of_Windows', 'Number_of_Doors','Traffic_Density_Score','Air_Quality_Index','Neighborhood_Review']\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform full pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "preprocessor.fit(train)\n",
    "train = preprocessor.transform(train)\n",
    "test = preprocessor.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "train = sc.fit_transform(train)\n",
    "test = sc.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = [\n",
    "    MLPRegressor(hidden_layer_sizes=(10,10,5,3),\n",
    "     batch_size =16, max_iter=1000, alpha=0.0001,\n",
    "     solver='adam', verbose=10, tol=0.000000001,\n",
    "     random_state=42, learning_rate_init=0.001)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model in turn\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Model: %s\" % model.__class__.__name__)\n",
    "    print(\"R-squared: %f\" % r2_score(y_test, y_pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPRegressor Tuning Parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'hidden_layer_sizes': [(10, 10, 3), (10, 10, 5, 3), (10, 10, 10, 5, 3)],\n",
    "    'max_iter': [100, 200, 500],\n",
    "    'alpha': [0.0001, 0.00001, 0.000001],\n",
    "    'solver': ['adam'],\n",
    "    'verbose': [10],\n",
    "    'tol': [0.000000001],\n",
    "    'random_state': [42],\n",
    "    'learning_rate_init': [0.001]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(MLPRegressor(), parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model\n",
    "final_model = MLPRegressor(hidden_layer_sizes=(10,10,3,),\n",
    "                        batch_size =64, max_iter=1000, \n",
    "                        alpha=0.0001, solver='adam',\n",
    "                        verbose=10, tol=0.000000001,\n",
    "                        random_state=42, learning_rate_init=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 781.55775643\n",
      "Iteration 2, loss = 78.40772235\n",
      "Iteration 3, loss = 59.04660687\n",
      "Iteration 4, loss = 48.34771579\n",
      "Iteration 5, loss = 42.07876031\n",
      "Iteration 6, loss = 38.07736835\n",
      "Iteration 7, loss = 34.95038514\n",
      "Iteration 8, loss = 32.61459311\n",
      "Iteration 9, loss = 30.82848419\n",
      "Iteration 10, loss = 29.24995556\n",
      "Iteration 11, loss = 27.92305152\n",
      "Iteration 12, loss = 26.92262093\n",
      "Iteration 13, loss = 26.14238046\n",
      "Iteration 14, loss = 25.55270525\n",
      "Iteration 15, loss = 25.06299340\n",
      "Iteration 16, loss = 24.70815809\n",
      "Iteration 17, loss = 24.39564046\n",
      "Iteration 18, loss = 24.18450050\n",
      "Iteration 19, loss = 23.98783572\n",
      "Iteration 20, loss = 23.84412882\n",
      "Iteration 21, loss = 23.74461229\n",
      "Iteration 22, loss = 23.65841592\n",
      "Iteration 23, loss = 23.49954758\n",
      "Iteration 24, loss = 23.36675074\n",
      "Iteration 25, loss = 23.28665083\n",
      "Iteration 26, loss = 23.21500162\n",
      "Iteration 27, loss = 23.11078132\n",
      "Iteration 28, loss = 23.03799981\n",
      "Iteration 29, loss = 23.02842031\n",
      "Iteration 30, loss = 22.94646766\n",
      "Iteration 31, loss = 22.84921202\n",
      "Iteration 32, loss = 22.79231646\n",
      "Iteration 33, loss = 22.71505449\n",
      "Iteration 34, loss = 22.67671119\n",
      "Iteration 35, loss = 22.61293035\n",
      "Iteration 36, loss = 22.61091457\n",
      "Iteration 37, loss = 22.53189324\n",
      "Iteration 38, loss = 22.46190192\n",
      "Iteration 39, loss = 22.44326446\n",
      "Iteration 40, loss = 22.42906305\n",
      "Iteration 41, loss = 22.39583162\n",
      "Iteration 42, loss = 22.35389013\n",
      "Iteration 43, loss = 22.30593947\n",
      "Iteration 44, loss = 22.30911270\n",
      "Iteration 45, loss = 22.23646066\n",
      "Iteration 46, loss = 22.24336522\n",
      "Iteration 47, loss = 22.21596744\n",
      "Iteration 48, loss = 22.18769503\n",
      "Iteration 49, loss = 22.13935781\n",
      "Iteration 50, loss = 22.14427171\n",
      "Iteration 51, loss = 22.12896359\n",
      "Iteration 52, loss = 22.09623290\n",
      "Iteration 53, loss = 22.12449009\n",
      "Iteration 54, loss = 22.06805746\n",
      "Iteration 55, loss = 22.09126111\n",
      "Iteration 56, loss = 22.07498329\n",
      "Iteration 57, loss = 22.01652791\n",
      "Iteration 58, loss = 22.05286905\n",
      "Iteration 59, loss = 22.00989780\n",
      "Iteration 60, loss = 21.97245607\n",
      "Iteration 61, loss = 22.03127933\n",
      "Iteration 62, loss = 21.99430785\n",
      "Iteration 63, loss = 21.99099326\n",
      "Iteration 64, loss = 21.97324673\n",
      "Iteration 65, loss = 21.98787349\n",
      "Iteration 66, loss = 21.95952858\n",
      "Iteration 67, loss = 21.95438940\n",
      "Iteration 68, loss = 21.92627816\n",
      "Iteration 69, loss = 21.95258013\n",
      "Iteration 70, loss = 21.98442314\n",
      "Iteration 71, loss = 21.90043899\n",
      "Iteration 72, loss = 21.89217075\n",
      "Iteration 73, loss = 21.91180375\n",
      "Iteration 74, loss = 21.87314044\n",
      "Iteration 75, loss = 21.91719038\n",
      "Iteration 76, loss = 21.89344057\n",
      "Iteration 77, loss = 21.86762856\n",
      "Iteration 78, loss = 21.88722081\n",
      "Iteration 79, loss = 21.86940025\n",
      "Iteration 80, loss = 21.85943694\n",
      "Iteration 81, loss = 21.85289788\n",
      "Iteration 82, loss = 21.82026314\n",
      "Iteration 83, loss = 21.81604290\n",
      "Iteration 84, loss = 21.82168174\n",
      "Iteration 85, loss = 21.79489823\n",
      "Iteration 86, loss = 21.81707437\n",
      "Iteration 87, loss = 21.79495660\n",
      "Iteration 88, loss = 21.78751060\n",
      "Iteration 89, loss = 21.77577784\n",
      "Iteration 90, loss = 21.75603384\n",
      "Iteration 91, loss = 21.77551860\n",
      "Iteration 92, loss = 21.74190840\n",
      "Iteration 93, loss = 21.72942680\n",
      "Iteration 94, loss = 21.73060544\n",
      "Iteration 95, loss = 21.71381923\n",
      "Iteration 96, loss = 21.71400963\n",
      "Iteration 97, loss = 21.69756995\n",
      "Iteration 98, loss = 21.66355800\n",
      "Iteration 99, loss = 21.66189941\n",
      "Iteration 100, loss = 21.60917566\n",
      "Iteration 101, loss = 21.60760723\n",
      "Iteration 102, loss = 21.54898757\n",
      "Iteration 103, loss = 21.54288336\n",
      "Iteration 104, loss = 21.48216873\n",
      "Iteration 105, loss = 21.50845816\n",
      "Iteration 106, loss = 21.44033848\n",
      "Iteration 107, loss = 21.44352106\n",
      "Iteration 108, loss = 21.33391359\n",
      "Iteration 109, loss = 21.31405490\n",
      "Iteration 110, loss = 21.23422849\n",
      "Iteration 111, loss = 21.20334204\n",
      "Iteration 112, loss = 21.20866111\n",
      "Iteration 113, loss = 21.16392295\n",
      "Iteration 114, loss = 21.13116519\n",
      "Iteration 115, loss = 21.12816993\n",
      "Iteration 116, loss = 21.10015932\n",
      "Iteration 117, loss = 21.08166595\n",
      "Iteration 118, loss = 21.09939223\n",
      "Iteration 119, loss = 21.07078428\n",
      "Iteration 120, loss = 21.02773436\n",
      "Iteration 121, loss = 21.00996750\n",
      "Iteration 122, loss = 21.04052941\n",
      "Iteration 123, loss = 21.00280522\n",
      "Iteration 124, loss = 21.00776122\n",
      "Iteration 125, loss = 20.97841797\n",
      "Iteration 126, loss = 20.97784246\n",
      "Iteration 127, loss = 20.85784190\n",
      "Iteration 128, loss = 20.76283866\n",
      "Iteration 129, loss = 20.73466996\n",
      "Iteration 130, loss = 20.70077555\n",
      "Iteration 131, loss = 20.66814998\n",
      "Iteration 132, loss = 20.67213132\n",
      "Iteration 133, loss = 20.62455148\n",
      "Iteration 134, loss = 20.59307278\n",
      "Iteration 135, loss = 20.56441792\n",
      "Iteration 136, loss = 20.58817620\n",
      "Iteration 137, loss = 20.57760035\n",
      "Iteration 138, loss = 20.60483537\n",
      "Iteration 139, loss = 20.54857400\n",
      "Iteration 140, loss = 20.55197127\n",
      "Iteration 141, loss = 20.51741556\n",
      "Iteration 142, loss = 20.54477896\n",
      "Iteration 143, loss = 20.47396365\n",
      "Iteration 144, loss = 20.49337267\n",
      "Iteration 145, loss = 20.46072983\n",
      "Iteration 146, loss = 20.48939877\n",
      "Iteration 147, loss = 20.42584211\n",
      "Iteration 148, loss = 20.53470126\n",
      "Iteration 149, loss = 20.42820510\n",
      "Iteration 150, loss = 20.39441053\n",
      "Iteration 151, loss = 20.42035325\n",
      "Iteration 152, loss = 20.46036913\n",
      "Iteration 153, loss = 20.38266767\n",
      "Iteration 154, loss = 20.40791826\n",
      "Iteration 155, loss = 20.41474737\n",
      "Iteration 156, loss = 20.38478056\n",
      "Iteration 157, loss = 20.38440181\n",
      "Iteration 158, loss = 20.37938707\n",
      "Iteration 159, loss = 20.38911677\n",
      "Iteration 160, loss = 20.38223408\n",
      "Iteration 161, loss = 20.38631598\n",
      "Iteration 162, loss = 20.34152414\n",
      "Iteration 163, loss = 20.38338501\n",
      "Iteration 164, loss = 20.35960479\n",
      "Iteration 165, loss = 20.35372392\n",
      "Iteration 166, loss = 20.36410918\n",
      "Iteration 167, loss = 20.36877410\n",
      "Iteration 168, loss = 20.34121956\n",
      "Iteration 169, loss = 20.33826191\n",
      "Iteration 170, loss = 20.32007235\n",
      "Iteration 171, loss = 20.29806450\n",
      "Iteration 172, loss = 20.30255319\n",
      "Iteration 173, loss = 20.31099868\n",
      "Iteration 174, loss = 20.26535404\n",
      "Iteration 175, loss = 20.27730024\n",
      "Iteration 176, loss = 20.26710250\n",
      "Iteration 177, loss = 20.26478117\n",
      "Iteration 178, loss = 20.25553651\n",
      "Iteration 179, loss = 20.18156039\n",
      "Iteration 180, loss = 20.19332748\n",
      "Iteration 181, loss = 20.14911195\n",
      "Iteration 182, loss = 20.11291490\n",
      "Iteration 183, loss = 20.10205717\n",
      "Iteration 184, loss = 20.08856215\n",
      "Iteration 185, loss = 20.04600860\n",
      "Iteration 186, loss = 20.05708517\n",
      "Iteration 187, loss = 20.00292655\n",
      "Iteration 188, loss = 20.00238814\n",
      "Iteration 189, loss = 19.94304065\n",
      "Iteration 190, loss = 19.93888947\n",
      "Iteration 191, loss = 19.89468426\n",
      "Iteration 192, loss = 19.85031371\n",
      "Iteration 193, loss = 19.69963156\n",
      "Iteration 194, loss = 19.67304590\n",
      "Iteration 195, loss = 19.58473198\n",
      "Iteration 196, loss = 19.52617547\n",
      "Iteration 197, loss = 19.45155659\n",
      "Iteration 198, loss = 19.40493634\n",
      "Iteration 199, loss = 19.32807578\n",
      "Iteration 200, loss = 19.24932557\n",
      "Iteration 201, loss = 19.26378050\n",
      "Iteration 202, loss = 19.21162926\n",
      "Iteration 203, loss = 19.16389189\n",
      "Iteration 204, loss = 19.10013220\n",
      "Iteration 205, loss = 19.04865015\n",
      "Iteration 206, loss = 19.04251898\n",
      "Iteration 207, loss = 19.01030818\n",
      "Iteration 208, loss = 19.00028286\n",
      "Iteration 209, loss = 18.95047086\n",
      "Iteration 210, loss = 18.97531569\n",
      "Iteration 211, loss = 18.97566944\n",
      "Iteration 212, loss = 18.89334867\n",
      "Iteration 213, loss = 18.91003404\n",
      "Iteration 214, loss = 18.90481238\n",
      "Iteration 215, loss = 18.88948384\n",
      "Iteration 216, loss = 18.86901663\n",
      "Iteration 217, loss = 18.85814642\n",
      "Iteration 218, loss = 18.86409765\n",
      "Iteration 219, loss = 18.82627219\n",
      "Iteration 220, loss = 18.81847396\n",
      "Iteration 221, loss = 18.81291213\n",
      "Iteration 222, loss = 18.86182530\n",
      "Iteration 223, loss = 18.80695972\n",
      "Iteration 224, loss = 18.78626100\n",
      "Iteration 225, loss = 18.80138657\n",
      "Iteration 226, loss = 18.76268512\n",
      "Iteration 227, loss = 18.78697029\n",
      "Iteration 228, loss = 18.76474639\n",
      "Iteration 229, loss = 18.75131775\n",
      "Iteration 230, loss = 18.76858844\n",
      "Iteration 231, loss = 18.78807640\n",
      "Iteration 232, loss = 18.72737242\n",
      "Iteration 233, loss = 18.74042350\n",
      "Iteration 234, loss = 18.74921726\n",
      "Iteration 235, loss = 18.74678226\n",
      "Iteration 236, loss = 18.75741663\n",
      "Iteration 237, loss = 18.74406892\n",
      "Iteration 238, loss = 18.72722768\n",
      "Iteration 239, loss = 18.72032088\n",
      "Iteration 240, loss = 18.72553681\n",
      "Iteration 241, loss = 18.71055129\n",
      "Iteration 242, loss = 18.71504904\n",
      "Iteration 243, loss = 18.69365794\n",
      "Iteration 244, loss = 18.66307688\n",
      "Iteration 245, loss = 18.71057050\n",
      "Iteration 246, loss = 18.67390025\n",
      "Iteration 247, loss = 18.69095150\n",
      "Iteration 248, loss = 18.65115011\n",
      "Iteration 249, loss = 18.64839134\n",
      "Iteration 250, loss = 18.66832728\n",
      "Iteration 251, loss = 18.64509662\n",
      "Iteration 252, loss = 18.67085838\n",
      "Iteration 253, loss = 18.61268072\n",
      "Iteration 254, loss = 18.66108495\n",
      "Iteration 255, loss = 18.62440946\n",
      "Iteration 256, loss = 18.63314944\n",
      "Iteration 257, loss = 18.63286170\n",
      "Iteration 258, loss = 18.59430033\n",
      "Iteration 259, loss = 18.61045728\n",
      "Iteration 260, loss = 18.60455528\n",
      "Iteration 261, loss = 18.59937003\n",
      "Iteration 262, loss = 18.61624265\n",
      "Iteration 263, loss = 18.59460555\n",
      "Iteration 264, loss = 18.61363649\n",
      "Iteration 265, loss = 18.59827502\n",
      "Iteration 266, loss = 18.59113190\n",
      "Iteration 267, loss = 18.56660137\n",
      "Iteration 268, loss = 18.59336690\n",
      "Iteration 269, loss = 18.57623993\n",
      "Iteration 270, loss = 18.57783405\n",
      "Iteration 271, loss = 18.54689128\n",
      "Iteration 272, loss = 18.58603495\n",
      "Iteration 273, loss = 18.57013611\n",
      "Iteration 274, loss = 18.55186076\n",
      "Iteration 275, loss = 18.60606865\n",
      "Iteration 276, loss = 18.59149188\n",
      "Iteration 277, loss = 18.53074158\n",
      "Iteration 278, loss = 18.55357786\n",
      "Iteration 279, loss = 18.53845285\n",
      "Iteration 280, loss = 18.54727790\n",
      "Iteration 281, loss = 18.54112676\n",
      "Iteration 282, loss = 18.55509198\n",
      "Iteration 283, loss = 18.56306811\n",
      "Iteration 284, loss = 18.51646815\n",
      "Iteration 285, loss = 18.53978785\n",
      "Iteration 286, loss = 18.53459058\n",
      "Iteration 287, loss = 18.55237625\n",
      "Iteration 288, loss = 18.54769703\n",
      "Iteration 289, loss = 18.53445115\n",
      "Iteration 290, loss = 18.54997413\n",
      "Iteration 291, loss = 18.55262093\n",
      "Iteration 292, loss = 18.53050774\n",
      "Iteration 293, loss = 18.50778435\n",
      "Iteration 294, loss = 18.55030601\n",
      "Iteration 295, loss = 18.51017601\n",
      "Iteration 296, loss = 18.50713860\n",
      "Iteration 297, loss = 18.49099281\n",
      "Iteration 298, loss = 18.47995801\n",
      "Iteration 299, loss = 18.50589443\n",
      "Iteration 300, loss = 18.49783954\n",
      "Iteration 301, loss = 18.50897656\n",
      "Iteration 302, loss = 18.48194283\n",
      "Iteration 303, loss = 18.48756189\n",
      "Iteration 304, loss = 18.48988903\n",
      "Iteration 305, loss = 18.49273615\n",
      "Iteration 306, loss = 18.48339438\n",
      "Iteration 307, loss = 18.46142809\n",
      "Iteration 308, loss = 18.49859794\n",
      "Iteration 309, loss = 18.47200930\n",
      "Iteration 310, loss = 18.46525577\n",
      "Iteration 311, loss = 18.47201723\n",
      "Iteration 312, loss = 18.48540384\n",
      "Iteration 313, loss = 18.48222508\n",
      "Iteration 314, loss = 18.48988619\n",
      "Iteration 315, loss = 18.45983121\n",
      "Iteration 316, loss = 18.46993345\n",
      "Iteration 317, loss = 18.48975221\n",
      "Iteration 318, loss = 18.46392461\n",
      "Iteration 319, loss = 18.43816818\n",
      "Iteration 320, loss = 18.44922409\n",
      "Iteration 321, loss = 18.46793720\n",
      "Iteration 322, loss = 18.45384752\n",
      "Iteration 323, loss = 18.45011808\n",
      "Iteration 324, loss = 18.44047894\n",
      "Iteration 325, loss = 18.44110334\n",
      "Iteration 326, loss = 18.41893019\n",
      "Iteration 327, loss = 18.41601452\n",
      "Iteration 328, loss = 18.44631850\n",
      "Iteration 329, loss = 18.44855449\n",
      "Iteration 330, loss = 18.43822313\n",
      "Iteration 331, loss = 18.41780369\n",
      "Iteration 332, loss = 18.41960648\n",
      "Iteration 333, loss = 18.42520094\n",
      "Iteration 334, loss = 18.43825537\n",
      "Iteration 335, loss = 18.43181039\n",
      "Iteration 336, loss = 18.43847880\n",
      "Iteration 337, loss = 18.42728013\n",
      "Iteration 338, loss = 18.39321400\n",
      "Iteration 339, loss = 18.40692134\n",
      "Iteration 340, loss = 18.40554770\n",
      "Iteration 341, loss = 18.42601169\n",
      "Iteration 342, loss = 18.36473425\n",
      "Iteration 343, loss = 18.43233775\n",
      "Iteration 344, loss = 18.41945149\n",
      "Iteration 345, loss = 18.43583881\n",
      "Iteration 346, loss = 18.41133497\n",
      "Iteration 347, loss = 18.37766949\n",
      "Iteration 348, loss = 18.39580391\n",
      "Iteration 349, loss = 18.39560043\n",
      "Iteration 350, loss = 18.38730785\n",
      "Iteration 351, loss = 18.39480369\n",
      "Iteration 352, loss = 18.41942003\n",
      "Iteration 353, loss = 18.40179500\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# make final predictions\n",
    "final_model.fit(train, y)\n",
    "final_predictions = final_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to file\n",
    "results = pd.DataFrame({'Property_ID': testPrpId, 'Habitability_score': final_predictions})\n",
    "\n",
    "filename = \"submission.csv\"\n",
    "\n",
    "if os.path.exists(filename):\n",
    "  os.remove(filename)\n",
    "results.to_csv(filename, index=False,header=True, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
